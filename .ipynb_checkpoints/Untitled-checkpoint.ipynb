{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# venv\\Scripts\\activate\n",
    "# cd C:\\Users\\felix\\Dropbox\\Uni-ty\\6. Semester\\Projektarbeit\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import dataCsvReader as dr\n",
    "import dataDownloader as dl\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import date\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "os.system('cls')\n",
    "                  #dataset, dataset[:, 0], 0,TRAIN_SPLIT, past_history,future_target, STEP)\n",
    "\n",
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i - history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        # data=np.append(data,dataset[i:i+target_size,1:],axis=1)\n",
    "        # print(data)\n",
    "        # print(\"********++\")\n",
    "        if single_step:\n",
    "            labels.append(target[i + target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i + target_size])\n",
    "    # print(\"dataset\")\n",
    "    # print(np.array(data))\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "\n",
    "\n",
    "def multi_step_plot(history, true_future, prediction):\n",
    "    # print(history)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    num_in = create_time_steps(len(history))\n",
    "    num_out = len(true_future)\n",
    "\n",
    "    plt.plot(num_in, np.array(history[:, 0]), label='History')\n",
    "    plt.plot(np.arange(num_out) / STEP, np.array(true_future), 'b',\n",
    "             label='True Future')\n",
    "    if prediction.any():\n",
    "        plt.plot(np.arange(num_out) / STEP, np.array(prediction), 'r',\n",
    "                 label='Predicted Future')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_time_steps(length):\n",
    "    time_steps = []\n",
    "    for i in range(-length, 0, 1):\n",
    "        time_steps.append(i)\n",
    "    return time_steps\n",
    "\n",
    "\n",
    "def plot_train_history(history, title):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(loss))\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_history = 48# inputtimesteps\n",
    "future_target = 24  # timesteps into future\n",
    "TEST_LENGTH= 12600 #Stunden\n",
    "STEP = 1\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000\n",
    "EPOCHS = 25\n",
    "\n",
    "\n",
    "start='2016-1-1'\n",
    "end=date.today()\n",
    "isTraining = True\n",
    "# isTraining=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if(False):\n",
    "  dl.updateWeatherHistory(start=start,end=end,times=[\"recent\"])\n",
    "  dl.updateForecast()\n",
    "  dl.updatePowerprice()\n",
    "data,forecastStart= dr.getData(start=start,end=end)\n",
    "# data = data.drop(['diffScaledPrice'], axis=1)\n",
    "data = data.drop(['Price'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.values\n",
    "TRAIN_SPLIT = len(dataset)-TEST_LENGTH\n",
    "# normalize the dataset using the mean and standard deviation of the training data\n",
    "#data_mean = dataset[:TRAIN_SPLIT].mean(axis=0)\n",
    "#data_std = dataset[:TRAIN_SPLIT].std(axis=0)\n",
    "#dataset = (dataset - data_mean) / data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions 22465 and 0 are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-b729212fc108>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                              \u001b[0mTRAIN_SPLIT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpast_history\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                              future_target, STEP)\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrain_data_multi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_multi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_multi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mtrain_data_multi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data_multi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_multi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\felix\\venv\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    433\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \"\"\"\n\u001b[1;32m--> 435\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m   \u001b[1;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\felix\\venv\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, element)\u001b[0m\n\u001b[0;32m   2362\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2363\u001b[0m       batch_dim.assert_is_compatible_with(tensor_shape.Dimension(\n\u001b[1;32m-> 2364\u001b[1;33m           tensor_shape.dimension_value(t.get_shape()[0])))\n\u001b[0m\u001b[0;32m   2365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2366\u001b[0m     variant_tensor = gen_dataset_ops.tensor_slice_dataset(\n",
      "\u001b[1;32mc:\\users\\felix\\venv\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m       raise ValueError(\"Dimensions %s and %s are not compatible\" %\n\u001b[1;32m--> 275\u001b[1;33m                        (self, other))\n\u001b[0m\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mmerge_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions 22465 and 0 are not compatible"
     ]
    }
   ],
   "source": [
    "def multivariate_data_Single_Step(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "    start_index = start_index + history_size\n",
    "    \n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "    for i in range(history_size, end_index):\n",
    "        indices = range(i - history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "        if single_step:\n",
    "            labels.append(target[i + target_size])\n",
    "    print(data,labels)\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "x_train_multi, y_train_multi = multivariate_data_Single_Step(dataset[:,1:], dataset[:, 0], 0,\n",
    "                                                 TRAIN_SPLIT, past_history,\n",
    "                                                 future_target, STEP)\n",
    "x_val_multi, y_val_multi = multivariate_data(dataset, dataset[:, 0],\n",
    "                                             TRAIN_SPLIT, None, past_history,\n",
    "                                             future_target, STEP)\n",
    "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
    "train_data_multi = train_data_multi.cache().shuffle(len(x_train_multi)).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
    "val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "multi_step_model = tf.keras.models.Sequential()\n",
    "multi_step_model.add(tf.keras.layers.GRU(past_history, return_sequences=True, input_shape=x_train_multi.shape[-2:]))\n",
    "multi_step_model.add(tf.keras.layers.GRU(past_history))\n",
    "# multi_step_model.add(tf.keras.layers.GRU(int(past_history/2), return_sequences=True))\n",
    "# multi_step_model.add(tf.keras.layers.GRU(int(past_history/2)))\n",
    "multi_step_model.add(tf.keras.layers.Dense(future_target))\n",
    "multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isTraining:\n",
    "    multi_step_history = multi_step_model.fit(train_data_multi, epochs=EPOCHS,\n",
    "                                          steps_per_epoch=6000,\n",
    "                                          validation_data=val_data_multi,\n",
    "                                          validation_steps=1000)\n",
    "    plot_train_history(multi_step_history, 'Multi-Step Training and validation loss')\n",
    "    multi_step_model.save_weights'('./checkpoints/testing')\n",
    "else:\n",
    "    multi_step_model.load_weights('./checkpoints/testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in val_data_multi.take(1):\n",
    "    multi_step_plot(x[0], y[0], multi_step_model.predict(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
